{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stas/Documents/University/Masters/semester 3/ILM/TeachingCase/ilm_p3_12/lib64/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "def llm(query, token, model):\n",
    "    \"\"\"\n",
    "    Query an LLM using the Hugging Face Inference API.\n",
    "\n",
    "    Parameters:\n",
    "        query (str): The input query.\n",
    "        token (str): Hugging Face API token.\n",
    "        model (str): Model.\n",
    "\n",
    "    Returns:\n",
    "        str: Generated response from the LLM.\n",
    "    \"\"\"\n",
    "    parameters = {\n",
    "        \"max_new_tokens\": 300,\n",
    "        \"temperature\": 0.1,\n",
    "        \"top_k\": 50,\n",
    "        \"top_p\": 0.95,\n",
    "        \"return_full_text\": False\n",
    "    }\n",
    "    \n",
    "    headers = {\n",
    "        'Authorization': f'Bearer {token}',\n",
    "        'Content-Type': 'application/json'\n",
    "    }\n",
    "    \n",
    "    payload = {\n",
    "        \"inputs\": query,\n",
    "        \"parameters\": parameters\n",
    "    }\n",
    "    \n",
    "    response = requests.post(f\"https://api-inference.huggingface.co/models/meta-llama/{model}\", headers=headers, json=payload)\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"API request failed with status {response.status_code}: {response.text}\")\n",
    "    \n",
    "    return response.json()[0].get('generated_text', '').strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = os.getenv(\"HUGGING_FACE_TOKEN\")\n",
    "model = \"Meta-Llama-3-8B-Instruct\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_shap_prompt(customer_data, prediction, prediction_proba):\n",
    "    prompt = \"\"\n",
    "    risk_label = \"high risk\" if prediction == 1 else \"low risk\"\n",
    "    features_and_shap_values = \"\"\n",
    "    \n",
    "    # Iterate through each feature in the SHAP data\n",
    "    for _, row in customer_data.iterrows():\n",
    "        feature = row[\"Feature\"]\n",
    "        feature_value = row[\"Feature Value\"]\n",
    "        shap_value = row[\"SHAP Value\"]\n",
    "    \n",
    "        # Here we add the SHAP and feature information\n",
    "        if isinstance(feature_value, (int, float)):\n",
    "            features_and_shap_values += f\"- {feature}: {feature_value:.2f} (SHAP impact: {shap_value:.2f})\\n\"\n",
    "        else:\n",
    "            features_and_shap_values += f\"- {feature}: {feature_value} (SHAP impact: {shap_value:.2f})\\n\"\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "You are a smart and helpful explainer and interpreter for a machine learning model that classifies customers as high or low risk regarding credit default.\n",
    "\n",
    "The customer in question has been classified as {risk_label} with a probability of {prediction_proba:.2f}.\n",
    "Below are the customer's features and their corresponding SHAP values:\n",
    "\n",
    "{features_and_shap_values}\n",
    "\n",
    "Instructions:\n",
    "1. Provide only the main reasons the customer was classified as {risk_label} by referencing the most impactful features and the respective reasons behind it.\n",
    "2. Do not include any disclaimers, contact information, or explanations of what SHAP values are. You should analyze the SHAP values in relation to the feature values and the connections between them, but the values themselves don't have to be mentioned in the reply. \n",
    "3. Write your answer without extra salutations, sign-offs and mentions of the SHAP values.\n",
    "4. Write your answer using bullet points for the features you want to mention.\n",
    "\n",
    "Take your time to thoroughly analyze the values and the connections. Note that the false flags for the status are because of the one hot encoding, meaning that the true flag was the one representative of the analyzed customer.\n",
    "Please explain the primary factors that led to this classification:\n",
    "\"\"\"\n",
    "\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "customer_index = 6799\n",
    "customer_data = load_shap_values(customer_index)\n",
    "formatted_prompt = format_shap_prompt(\n",
    "    customer_data, \n",
    "    prediction=1,  # customer classified as high risk\n",
    "    prediction_proba=0.85)\n",
    "\n",
    "# print(formatted_prompt)\n",
    "\n",
    "try:\n",
    "    response = llm(formatted_prompt, token, model)\n",
    "    print(\"Generated Response:\\n\", response)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ilm_p3_12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
