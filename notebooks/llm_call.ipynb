{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BianchiLuca28/XAI-Healthcare/blob/main/notebooks/llm_call.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports and setup"
      ],
      "metadata": {
        "id": "3CHg8fKs3xL2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-dotenv"
      ],
      "metadata": {
        "id": "vwLljSRv30L5",
        "outputId": "a42c7e6d-41aa-4515-cfba-94856c35be7a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.11/dist-packages (1.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "zRNOWG7Ez6i9"
      },
      "outputs": [],
      "source": [
        "from dotenv import load_dotenv\n",
        "import os\n",
        "import requests"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ymWTkQOBz6jA"
      },
      "source": [
        "# Setting up LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "XmU1CJu3z6jC"
      },
      "outputs": [],
      "source": [
        "def llm(query, token, model):\n",
        "    \"\"\"\n",
        "    Query an LLM using the Hugging Face Inference API.\n",
        "\n",
        "    Parameters:\n",
        "        query (str): The input query.\n",
        "        token (str): Hugging Face API token.\n",
        "        model (str): Model.\n",
        "\n",
        "    Returns:\n",
        "        str: Generated response from the LLM.\n",
        "    \"\"\"\n",
        "    parameters = {\n",
        "        \"max_new_tokens\": 300,\n",
        "        \"temperature\": 0.1,\n",
        "        \"top_k\": 50,\n",
        "        \"top_p\": 0.95,\n",
        "        \"return_full_text\": False\n",
        "    }\n",
        "\n",
        "    headers = {\n",
        "        'Authorization': f'Bearer {token}',\n",
        "        'Content-Type': 'application/json'\n",
        "    }\n",
        "\n",
        "    payload = {\n",
        "        \"inputs\": query,\n",
        "        \"parameters\": parameters\n",
        "    }\n",
        "\n",
        "    response = requests.post(f\"https://api-inference.huggingface.co/models/{model}\", headers=headers, json=payload)\n",
        "\n",
        "    if response.status_code != 200:\n",
        "        raise Exception(f\"API request failed with status {response.status_code}: {response.text}\")\n",
        "\n",
        "    return response.json()[0].get('generated_text', '').strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "PbcpnLp7z6jE",
        "outputId": "ad310f3d-2137-4680-8da4-77da6698bb81",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "load_dotenv()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "NosjCFS6z6jE"
      },
      "outputs": [],
      "source": [
        "# token = \"HUGGING_FACE\"\n",
        "model = \"meta-llama/Llama-3.1-8B-Instruct\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D3aw60a2z6jF"
      },
      "source": [
        "# Prompting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "cellView": "code",
        "id": "e5KAeBjZz6jF"
      },
      "outputs": [],
      "source": [
        "def format_shap_prompt(customer_data, prediction, prediction_proba):\n",
        "    prompt = \"\"\n",
        "    risk_label = \"high risk\" if prediction == 1 else \"low risk\"\n",
        "    features_and_shap_values = \"\"\n",
        "\n",
        "    # Iterate through each feature in the SHAP data\n",
        "    for _, row in customer_data.iterrows():\n",
        "        feature = row[\"Feature\"]\n",
        "        feature_value = row[\"Feature Value\"]\n",
        "        shap_value = row[\"SHAP Value\"]\n",
        "\n",
        "        # Here we add the SHAP and feature information\n",
        "        if isinstance(feature_value, (int, float)):\n",
        "            features_and_shap_values += f\"- {feature}: {feature_value:.2f} (SHAP impact: {shap_value:.2f})\\n\"\n",
        "        else:\n",
        "            features_and_shap_values += f\"- {feature}: {feature_value} (SHAP impact: {shap_value:.2f})\\n\"\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "You are a smart and helpful explainer and interpreter for a machine learning model that classifies customers as high or low risk regarding credit default.\n",
        "\n",
        "The customer in question has been classified as {risk_label} with a probability of {prediction_proba:.2f}.\n",
        "Below are the customer's features and their corresponding SHAP values:\n",
        "\n",
        "{features_and_shap_values}\n",
        "\n",
        "Instructions:\n",
        "1. Provide only the main reasons the customer was classified as {risk_label} by referencing the most impactful features and the respective reasons behind it.\n",
        "2. Do not include any disclaimers, contact information, or explanations of what SHAP values are. You should analyze the SHAP values in relation to the feature values and the connections between them, but the values themselves don't have to be mentioned in the reply.\n",
        "3. Write your answer without extra salutations, sign-offs and mentions of the SHAP values.\n",
        "4. Write your answer using bullet points for the features you want to mention.\n",
        "\n",
        "Take your time to thoroughly analyze the values and the connections. Note that the false flags for the status are because of the one hot encoding, meaning that the true flag was the one representative of the analyzed customer.\n",
        "Please explain the primary factors that led to this classification:\n",
        "\"\"\"\n",
        "\n",
        "    return prompt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = f\"\"\"\n",
        "You are a smart and helpful explainer and interpreter for a machine learning model that explains Myocardial infarction complications.\n",
        "\n",
        "Tell me everything you know about it\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "l7m-_CL34WRb"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "jW2zfwLTz6jF",
        "outputId": "8b961c9e-759a-4ec2-c723-5900fa8b6ce6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "API request failed with status 400: {\"error\":\"Model requires a Pro subscription; check out hf.co/pricing to learn more. Make sure to include your HF token in your query.\"}\n"
          ]
        }
      ],
      "source": [
        "# formatted_prompt = format_shap_prompt(\n",
        "#     customer_data,\n",
        "#     prediction=1,  # customer classified as high risk\n",
        "#     prediction_proba=0.85)\n",
        "\n",
        "# print(formatted_prompt)\n",
        "\n",
        "try:\n",
        "    response = llm(prompt, token, model)\n",
        "    print(\"Generated Response:\\n\", response)\n",
        "except Exception as e:\n",
        "    print(e)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "ilm_p3_12",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}